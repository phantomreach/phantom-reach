{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNv0NoV38jjHXhd+a0hTb81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/phantom-reach/blob/main/Image_detection_Model_Comparasion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model         | MPII PCKh (%) | COCO Keypoint AP (%) | Speed             | Strengths                                     |\n",
        "|---------------|---------------|-----------------------|-------------------|-----------------------------------------------|\n",
        "| Mediapipe     | ~70–80*       | ~60*                 | Excellent (Real-time) | Lightweight, deployable on edge devices.       |\n",
        "| ViTPose       | ~90+          | 78–82                | Moderate          | Transformer-based, high precision.            |\n",
        "| HigherHRNet   | ~91           | 75–80                | Moderate          | Excellent for detailed single-person poses.   |\n",
        "| Lite-HRNet    | ~85           | 65–70                | Good              | Efficient and lightweight for real-time.      |\n",
        "| PoseNet       | ~80           | ~60–65               | Good              | Simple, good for single-person tasks.         |\n"
      ],
      "metadata": {
        "id": "tVHei7eqWwq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dependencies**"
      ],
      "metadata": {
        "id": "1pHUzyFjIrvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gPhsrGL4V4S"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install --upgrade pip\n",
        "!pip install fiftyone\n",
        "!pip install mediapipe\n",
        "!pip install opencv-python-headless\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13.0/index.html\n",
        "!pip install mmdet==2.28.1\n",
        "!pip install mmpose==0.29.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "d68wC4T3I7U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import fiftyone as fo\n",
        "import fiftyone.utils.huggingface as fouh\n",
        "import mediapipe as mp\n",
        "from mmpose.apis import init_pose_model, inference_top_down_pose_model\n",
        "from mmpose.datasets import DatasetInfo\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import mmcv\n",
        "import mmdet\n",
        "import mmpose"
      ],
      "metadata": {
        "id": "o0ArPrgn4eIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Preprocess the Dataset - Filtering relevant pictures**"
      ],
      "metadata": {
        "id": "4phPX9aMI_gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset name\n",
        "dataset_name = \"Voxel51/MPII_Human_Pose_Dataset\"\n",
        "\n",
        "# Check if the dataset exists and delete it\n",
        "if dataset_name in fo.list_datasets():\n",
        "    fo.delete_dataset(dataset_name)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = fouh.load_from_hub(\"Voxel51/MPII_Human_Pose_Dataset\")\n",
        "print(f\"Total dataset size: {len(dataset)} samples.\")"
      ],
      "metadata": {
        "id": "eflJqoYBmJBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_single_person_facing_camera(sample):\n",
        "    \"\"\"\n",
        "    Filters images containing a single person facing the camera.\n",
        "\n",
        "    Args:\n",
        "        sample: A single dataset sample.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the sample meets the criteria, False otherwise.\n",
        "    \"\"\"\n",
        "    if sample.annopoints and sample.annopoints.keypoints and len(sample.annopoints.keypoints) == 1:\n",
        "        kp = sample.annopoints.keypoints[0]\n",
        "        points = kp.points\n",
        "\n",
        "        # Ensure we have enough joints\n",
        "        if len(points) > 13:\n",
        "            # Extract keypoints of interest\n",
        "            left_shoulder = points[13]\n",
        "            right_shoulder = points[12]\n",
        "            nose = points[0] if len(points) > 0 else None\n",
        "\n",
        "            # Calculate distance between shoulders\n",
        "            shoulder_distance = abs(left_shoulder[0] - right_shoulder[0])\n",
        "\n",
        "            # Check for valid shoulder distance range\n",
        "            if 25 < shoulder_distance < 75:\n",
        "                # Ensure left shoulder is to the left of the right shoulder\n",
        "                if left_shoulder[0] < right_shoulder[0]:\n",
        "                    # Validate nose alignment (if available)\n",
        "                    if nose:\n",
        "                        nose_centered = left_shoulder[0] < nose[0] < right_shoulder[0]\n",
        "                        if not nose_centered:\n",
        "                            return False\n",
        "\n",
        "                    # If all conditions are met, return True\n",
        "                    return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "HSzaMbBjOubj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the filter\n",
        "filtered_ids = [\n",
        "    sample.id for sample in filtered_dataset if filter_single_person_facing_camera(sample)\n",
        "]\n",
        "filtered_dataset_camera_facing = filtered_dataset.select(filtered_ids)\n",
        "print(f\"Filtered dataset size (single person facing camera): {len(filtered_dataset_camera_facing)}\")\n",
        "# Visualize results\n",
        "visualize_samples(filtered_dataset_camera_facing, num_samples=10)"
      ],
      "metadata": {
        "id": "eyc8EMXkmK6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export filtered dataset with keypoints annotations\n",
        "filtered_dir = \"filtered_mpii\"\n",
        "if not os.path.exists(filtered_dir):\n",
        "    filtered_dataset.export(\n",
        "        export_dir=filtered_dir,\n",
        "        dataset_type=fo.types.FiftyOneDataset,\n",
        "        label_field=\"annopoints\",  # Ensure keypoints are included\n",
        "    )\n",
        "print(\"Filtered dataset exported.\")"
      ],
      "metadata": {
        "id": "wV4nbM-SmNDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Classes and Evaluation Function**"
      ],
      "metadata": {
        "id": "cbn623MKJcvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, dest_path):\n",
        "    if not os.path.exists(dest_path):\n",
        "        print(f\"Downloading {url} to {dest_path}...\")\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        open(dest_path, 'wb').write(r.content)\n",
        "        print(\"Download completed.\")\n",
        "    else:\n",
        "        print(f\"{dest_path} already exists.\")\n",
        "\n",
        "# MediaPipe Pose Model\n",
        "class MediaPipePoseModel:\n",
        "    def __init__(self):\n",
        "        self.mp_pose = mp.solutions.pose\n",
        "        self.pose = self.mp_pose.Pose(static_image_mode=True)\n",
        "\n",
        "    def predict(self, image):\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        results = self.pose.process(image_rgb)\n",
        "        if results.pose_landmarks:\n",
        "            keypoints = []\n",
        "            for lm in results.pose_landmarks.landmark:\n",
        "                x = lm.x * image.shape[1]\n",
        "                y = lm.y * image.shape[0]\n",
        "                visibility = lm.visibility\n",
        "                keypoints.append([x, y, visibility])\n",
        "            return np.array(keypoints)\n",
        "        return np.array([])\n",
        "\n",
        "# Lite-HRNet Model using MMPose\n",
        "class LiteHRNetModel:\n",
        "    def __init__(self):\n",
        "        # Download the config and checkpoint files\n",
        "        config_url = 'https://raw.githubusercontent.com/open-mmlab/mmpose/master/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/litehrnet/coco/litehrnet_30_coco_256x192.py'\n",
        "        checkpoint_url = 'https://download.openmmlab.com/mmpose/top_down/litehrnet/litehrnet_30_coco_256x192-4bdb48f9_20210423.pth'\n",
        "        config_path = 'litehrnet_30_coco_256x192.py'\n",
        "        checkpoint_path = 'litehrnet_30_coco_256x192.pth'\n",
        "        download_file(config_url, config_path)\n",
        "        download_file(checkpoint_url, checkpoint_path)\n",
        "\n",
        "        # Initialize the pose model\n",
        "        self.model = init_pose_model(config_path, checkpoint_path, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.dataset_info = self.model.cfg.data['test'].get('dataset_info', None)\n",
        "        if self.dataset_info is None:\n",
        "            raise ValueError(\"Dataset info is missing in the config file.\")\n",
        "        else:\n",
        "            self.dataset_info = DatasetInfo(self.dataset_info)\n",
        "\n",
        "        # Initialize a person detector (YOLOv5)\n",
        "        detector_config_url = 'https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/faster_rcnn/faster_rcnn_r50_fpn_coco.py'\n",
        "        detector_checkpoint_url = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "        detector_config_path = 'faster_rcnn_r50_fpn_coco.py'\n",
        "        detector_checkpoint_path = 'faster_rcnn_r50_fpn_coco.pth'\n",
        "        download_file(detector_config_url, detector_config_path)\n",
        "        download_file(detector_checkpoint_url, detector_checkpoint_path)\n",
        "\n",
        "        self.detector = init_detector(detector_config_path, detector_checkpoint_path, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def predict(self, image):\n",
        "        # Detect people in the image\n",
        "        mmdet_results = inference_detector(self.detector, image)\n",
        "        # Keep only person class (class_id = 0)\n",
        "        person_results = []\n",
        "        for bbox in mmdet_results[0]:\n",
        "            if bbox[4] >= 0.5:  # Confidence threshold\n",
        "                person_results.append({'bbox': bbox[:4]})\n",
        "\n",
        "        if not person_results:\n",
        "            return np.array([])\n",
        "\n",
        "        # Run pose estimation\n",
        "        pose_results, _ = inference_top_down_pose_model(\n",
        "            self.model,\n",
        "            image,\n",
        "            person_results,\n",
        "            bbox_thr=0.5,\n",
        "            format='xyxy',\n",
        "            dataset=self.model.cfg.data['test']['type'],\n",
        "            dataset_info=self.dataset_info,\n",
        "            return_heatmap=False,\n",
        "            outputs=None)\n",
        "\n",
        "        if pose_results:\n",
        "            # Assuming single person\n",
        "            keypoints = pose_results[0]['keypoints']\n",
        "            return keypoints\n",
        "        return np.array([])\n",
        "\n",
        "# ViTPose Model using MMPose\n",
        "class ViTPoseModel:\n",
        "    def __init__(self):\n",
        "        # Download the config and checkpoint files\n",
        "        config_url = 'https://raw.githubusercontent.com/ViTAE-Transformer/ViTPose/main/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/vitpose_base_coco_256x192.py'\n",
        "        checkpoint_url = 'https://github.com/ViTAE-Transformer/ViTPose/releases/download/v0.0.1/vitpose-b-multi-coco.pth'\n",
        "        config_path = 'vitpose_base_coco_256x192.py'\n",
        "        checkpoint_path = 'vitpose_base_coco.pth'\n",
        "        download_file(config_url, config_path)\n",
        "        download_file(checkpoint_url, checkpoint_path)\n",
        "\n",
        "        # Initialize the pose model\n",
        "        self.model = init_pose_model(config_path, checkpoint_path, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.dataset_info = self.model.cfg.data['test'].get('dataset_info', None)\n",
        "        if self.dataset_info is None:\n",
        "            raise ValueError(\"Dataset info is missing in the config file.\")\n",
        "        else:\n",
        "            self.dataset_info = DatasetInfo(self.dataset_info)\n",
        "\n",
        "        # Initialize a person detector\n",
        "        detector_config_url = 'https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/faster_rcnn/faster_rcnn_r50_fpn_coco.py'\n",
        "        detector_checkpoint_url = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "        detector_config_path = 'faster_rcnn_r50_fpn_coco.py'\n",
        "        detector_checkpoint_path = 'faster_rcnn_r50_fpn_coco.pth'\n",
        "        download_file(detector_config_url, detector_config_path)\n",
        "        download_file(detector_checkpoint_url, detector_checkpoint_path)\n",
        "\n",
        "        self.detector = init_detector(detector_config_path, detector_checkpoint_path, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def predict(self, image):\n",
        "        # Detect people in the image\n",
        "        mmdet_results = inference_detector(self.detector, image)\n",
        "        # Keep only person class (class_id = 0)\n",
        "        person_results = []\n",
        "        for bbox in mmdet_results[0]:\n",
        "            if bbox[4] >= 0.5:  # Confidence threshold\n",
        "                person_results.append({'bbox': bbox[:4]})\n",
        "\n",
        "        if not person_results:\n",
        "            return np.array([])\n",
        "\n",
        "        # Run pose estimation\n",
        "        pose_results, _ = inference_top_down_pose_model(\n",
        "            self.model,\n",
        "            image,\n",
        "            person_results,\n",
        "            bbox_thr=0.5,\n",
        "            format='xyxy',\n",
        "            dataset=self.model.cfg.data['test']['type'],\n",
        "            dataset_info=self.dataset_info,\n",
        "            return_heatmap=False,\n",
        "            outputs=None)\n",
        "\n",
        "        if pose_results:\n",
        "            # Assuming single person\n",
        "            keypoints = pose_results[0]['keypoints']\n",
        "            return keypoints\n",
        "        return np.array([])\n",
        "\n",
        "def compute_head_size(gt_keypoints):\n",
        "    \"\"\"\n",
        "    Computes the head size based on the distance between head top and upper neck.\n",
        "    \"\"\"\n",
        "    head_top = next((kp for kp in gt_keypoints if kp[2] == 9), None)\n",
        "    upper_neck = next((kp for kp in gt_keypoints if kp[2] == 8), None)\n",
        "    if head_top is not None and upper_neck is not None:\n",
        "        head_size = np.linalg.norm([head_top[0] - upper_neck[0], head_top[1] - upper_neck[1]])\n",
        "    else:\n",
        "        head_size = 1.0  # Default value if head keypoints are missing\n",
        "    return head_size\n",
        "\n",
        "def pckh(predictions, ground_truths, head_sizes):\n",
        "    \"\"\"\n",
        "    Computes the Percentage of Correct Keypoints normalized by head size (PCKh).\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for pred, gt, head_size in zip(predictions, ground_truths, head_sizes):\n",
        "        if pred.shape[0] == 0 or gt.shape[0] == 0:\n",
        "            continue\n",
        "        # Align keypoints by id\n",
        "        pred_dict = {i: kp for i, kp in enumerate(pred)}\n",
        "        gt_dict = {int(kp[2]): kp[:2] for kp in gt}\n",
        "        matched_ids = set(pred_dict.keys()).intersection(set(gt_dict.keys()))\n",
        "        distances = []\n",
        "        for idx in matched_ids:\n",
        "            pred_kp = pred_dict[idx][:2]\n",
        "            gt_kp = gt_dict[idx]\n",
        "            distance = np.linalg.norm(pred_kp - gt_kp) / head_size\n",
        "            distances.append(distance)\n",
        "            if distance < 0.5:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "def evaluate_model(model, dataset_dir):\n",
        "    predictions, ground_truths, inference_times, head_sizes = [], [], [], []\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = fo.Dataset.from_dir(\n",
        "        dataset_dir=dataset_dir,\n",
        "        dataset_type=fo.types.FiftyOneDataset,\n",
        "    )\n",
        "\n",
        "    for sample in tqdm(dataset):\n",
        "        image_path = sample.filepath\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Access ground truth keypoints\n",
        "        gt_annopoints = sample.annopoints[0]\n",
        "        gt_keypoints = []\n",
        "        for kp in gt_annopoints.keypoints:\n",
        "            gt_keypoints.append([kp.x, kp.y, kp.id])\n",
        "        gt_keypoints = np.array(gt_keypoints)\n",
        "\n",
        "        # Compute head size (distance between head top and upper neck)\n",
        "        head_size = compute_head_size(gt_keypoints)\n",
        "\n",
        "        start_time = time.time()\n",
        "        pred_keypoints = model.predict(image)\n",
        "        end_time = time.time()\n",
        "\n",
        "        predictions.append(pred_keypoints)\n",
        "        ground_truths.append(gt_keypoints)\n",
        "        head_sizes.append(head_size)\n",
        "        inference_times.append(end_time - start_time)\n",
        "\n",
        "    avg_time = np.mean(inference_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else float(\"inf\")\n",
        "    accuracy = pckh(predictions, ground_truths, head_sizes)\n",
        "\n",
        "    return {\"PCKh\": accuracy, \"FPS\": fps}\n"
      ],
      "metadata": {
        "id": "jX-rxc9qJIeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    dataset_dir = \"filtered_mpii\"\n",
        "\n",
        "    # Initialize models\n",
        "    print(\"Initializing MediaPipePoseModel...\")\n",
        "    mediapipe_model = MediaPipePoseModel()\n",
        "\n",
        "    print(\"Initializing LiteHRNetModel...\")\n",
        "    lite_hrnet_model = LiteHRNetModel()\n",
        "\n",
        "    print(\"Initializing ViTPoseModel...\")\n",
        "    vitpose_model = ViTPoseModel()\n",
        "\n",
        "    # Evaluate each model\n",
        "    print(\"Evaluating MediaPipe...\")\n",
        "    mediapipe_metrics = evaluate_model(mediapipe_model, dataset_dir)\n",
        "    print(\"MediaPipe Metrics:\", mediapipe_metrics)\n",
        "\n",
        "    print(\"Evaluating Lite-HRNet...\")\n",
        "    lite_hrnet_metrics = evaluate_model(lite_hrnet_model, dataset_dir)\n",
        "    print(\"Lite-HRNet Metrics:\", lite_hrnet_metrics)\n",
        "\n",
        "    print(\"Evaluating ViTPose...\")\n",
        "    vitpose_metrics = evaluate_model(vitpose_model, dataset_dir)\n",
        "    print(\"ViTPose Metrics:\", vitpose_metrics)\n"
      ],
      "metadata": {
        "id": "B3oibFShmqZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}